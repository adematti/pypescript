.. _user-scripting:

Scripting
=========

Example
-------

Let's say we want to construct a joint likelihood of two data vectors ``data1`` and ``data2``, with their own models ``model1`` and ``model2``
and a common covariance matrix ``cov``. Our parameter ``.yaml`` file would then be:

.. code-block:: yaml

  main:
    $modules: [like]

  like:
    $module_name: template_lib.likelihood
    $module_class: JointGaussianLikelihood
    join: [like1, like2]
    $modules: [cov]

  like1:
    $module_name: template_lib.likelihood
    $module_class: BaseLikelihood
    $modules: [data1, model1]

  like2:
    $module_name: template_lib.likelihood
    $module_class: BaseLikelihood
    $modules: [data2, model2]

  data1:
    $module_name: template_lib.data_vector
    # details about how to get the data vector #1

  model1:
    # details about model #1

  data2:
    $module_name: template_lib.data_vector
    # details about how to get the data vector #2

  model2:
   # details about model #2

  cov:
    $module_name: template_lib.covariance
    # details about how to get the common covariance


Here module names (left aligned) can be of any name, except ``main`` which is the entry to the pipeline.
The fields ``$modules`` provided for :class:`~pypescript.module.BasePipeline` inherited-modules  (called (sub)pipelines in the following)
list the modules they run.

.. note::

  All **pypescript** keywords start witht the Dollar sign $

One can achieve the same pipeline in Python with (for a theory model called :mod:`~template_lib.model.FlatModel`):

.. code-block:: python

  from pypescript import BaseModule, BasePipeline, ConfigBlock, SectionBlock
  from template_lib.model import FlatModel
  from template_lib.likelihood import BaseLikelihood, JointGaussianLikelihood

  config_block = ConfigBlock(config_fn)
  data1 = BaseModule.from_filename(name='data1',options=SectionBlock(config_block,'data1'))
  model1 = FlatModel(name='model1')
  data2 = BaseModule.from_filename(name='data2',options=SectionBlock(config_block,'data2'))
  model2 = BaseModule.from_filename(name='model2',options=SectionBlock(config_block,'model2'))
  cov = BaseModule.from_filename(name='cov',options=SectionBlock(config_block,'cov'))
  like1 = BaseLikelihood(name='like1',modules=[data1,model1])
  like2 = BaseLikelihood(name='like2',modules=[data2,model2])
  like = JointGaussianLikelihood(name='like',join=[like1,like2],modules=[cov])
  pipeline = BasePipeline(modules=[like])

Here ``options`` can be simple dictionaries (:class:`~pypescript.block.SectionBlock` simply takes a slice of ``config_block`` for the given section).
You can also modify each module's ``data_block`` at your will.
There is really no more complexity than using Python classes successively, with **pypescript** holding all these classes.
So even if you do not like the **pypescript** framework, you can still use **pypescript** modules very easily in your own code.

We also provide a wrapper that allows lighter syntax:

.. code-block:: python

  from pypescript import mimport
  model1 = mimport('template_lib.model',module_class='FlatModel',name='model1')

In diagrammatic representation (generated by :meth:`~pypescript.module.BasePipeline.plot_pipeline_graph`):

  .. image:: ../static/pipe3.png

**pypescript** rules
--------------------

The **pypescript** framework is agnostic about the actual operations performed by the modules it sets up, executes and cleans up.
This is key to ensuring the base code does not need to be modified when adding a new module.

Similarly, modules are agnostic about the operations performed by other modules.
This is key to ensuring modules do not need to be modified when adding new ones.

Hence, the pipeline integrity is ensured by the user script.
The main difficulty is to ensure that each module takes the input of the preceding module at the relevant entry ``(section, name)``
of ``data_block``, the :class:`~pypescript.block.DataBlock` instance passed to all modules (see :ref:`user-framework`).

CosmoSIS implements a linear pipeline: all modules form a single chain.
Instead, we allow for a tree-like structure, which is explored depth-first, left to right.
Both approaches would be fully equivalent if the ``data_block`` were a global variable for all modules.
Instead, contrary to CosmoSIS, each (sub)pipeline creates (at initialisation only) a (shallow!) datablock_copy of the ``data_block`` to be passed to its modules.

.. note::

  In the example above, ``[model2]`` does not know anything about ``[model1]`` products. If one wanted to add a common calculation beforehand
  (e.g. linear power spectrum), it would be added at the head of the ``modules`` list of ``[main]``
  (not of ``[like]`` because of the peculiar structure of :class:`~template_lib.likelihood.JointGaussianLikelihood` - its ``modules`` being run *after* ``join``).

Hence, any change made these modules to the ``data_block`` are local (effective within the (sub)pipeline), which we think is the most commmon expected behaviour.
Therefore, a precomputation performed ahead of this (sub)pipeline, saved into ``data_block[section,name]`` will not be erased by the
modules of this (sub)pipeline even if they write in the same entry of ``data_block``.
This allows modules to *update* (for them) previous entries in ``data_block`` and hence to keep a short list of entries ``(section, name)`` in use.
Then, most of the links between module input and output entries is encoded in the pipeline structure itself.
We think it also makes the pipeline structure more readable.
Yet, this may not be sufficient in some corner cases; we may e.g. want to save the result of a given operation (e.g. derived parameter)
performed at some position in the tree. This is made possible by using the keyword ``datablock_copy`` in any module section of the configuration file/dictionary::

  $datablock_duplicate:
    section2.name2: section1.name1

will (shallow!) copy the element from ``data_block`` entry ``(section1, name1)`` to entry ``(section2, name2)`` at each step (``setup``, ``execute``, ``cleanup``).
There is a global (i.e. shared by all modules whatever their depth) section: 'common'. So taking ``section2 = 'common'`` will make the element accessible anywhere in the pipeline.

One can also locally (i.e. in one module or subpipeline) map a ``data_block`` entry ``(section1, name1)`` to entry ``(section2, name2)``::

  $datablock_mapping:
    section2.name2: section1.name1

Unlike ``datablock_duplicate``, this works as a reference: any change in the value pointed by entry ``(section1, name1)`` is visible by ``(section2, name2)``.
This can be useful in case one wants to cast modules-specific parameters to their standard name within the relevant modules.

Eventually, one can locally set ``data_block`` entries using e.g.::

  $datablock_set:
    section2.name2: 42

To summarise:
  - we allow for a tree-like structure
  - any change to ``data_block`` is local within a given (sub)pipeline
  - the section where changes are global (effective for the whole pipeline) is 'common'
  - if necessary, any entry of ``data_block`` can be moved anywhere (including the 'global' sections) with the keyword ``datablock_duplicate`` in the configuration file/dictionary
  - ``config_block`` is always global.

.. note::

  Our framework is therefore a generalisation of the CosmoSIS structure.
  Therefore, one can always stick to the CosmoSIS structure if more intuitive.

.. note::

  It is left to the user not to generate module reference loops in their pipeline.

.. note::

  The ``execute`` function of each module is called at *each iteration*. This meaning depends on the context.
  If your (sub)pipeline performs an MCMC sampling, for example, then the top ``execute`` of this pipeline will be called at each MCMC step.
  But we can imagine that we loop on different data vectors instead. In this case, ``execute`` will be called for each of this vector.
  For example, we want to estimate the power spectrum of a mock catalogue, then perform cosmological inference.
  Our top base pipeline would run the modules corresponding to the power spectrum estimator, and the sampler.
  One could also imagine generating mocks before estimating their power spectrum, etc.

Configuration file shortcuts
----------------------------

For rapid and convenient scripting, a number of configuration file shortcuts have been defined.

Replacements
^^^^^^^^^^^^
One can refer to values define in any part of the configuration file through the syntax ``${section1.section2...}``, e.g.:

.. code-block:: yaml

  answer:
    to: 42

  the: 84

  ultimate:
    question: ${answer.to}
    of: ${the}

Here ``${answer.to}`` will be replaced by 42, and ``${the}`` by 84.
Note that since ``(section, name)`` only fields are retained, the original ``the`` entry will be discarded in the rest of the pipeline.
One can also refer to another configuration file, using the syntax: ``${path_to_other_file:answer.to}:``.

Imports
^^^^^^^
.. code-block:: yaml

  answer:
    to: 42

  the: 84

  ultimate:
    ${answer}:
    to: 21
    of: ${the}

Here ``utimate`` will be filled with the elements of ``answer`` (``to: 42``), then ``ultimate.to`` will be replaced by 21.
One can also import a section from another configuration file, using the syntax: ``${path_to_other_file:section}:``.
To import the other configuration file completely, no section is specified: ``${path_to_other_file:}:``.

Mapping (references)
^^^^^^^^^^^^^^^^^^^^
``config_block`` entries can be mapped to each other through the syntax ``$&{section.name}``, e.g.:

.. code-block:: yaml

  answer:
    to: 42

  ultimate:
    question: $&{answer.to}

Here the ``config_block`` entry ``(ultimate, question)`` will refer to ``(answer, to)`` (meaning any change to the latter in the process of the pipeline will affect as well the former).

Eval pattern
^^^^^^^^^^^^
In some cases we may want to directly evaluate some Python code (e.g. comprehension list).
The syntax is ``e''``:

.. code-block:: yaml

  answer:
    to: 42

  ultimate:
    question: e'[${answer.to} + i for i in range(10)]'

The entry ``(ultimate, question)`` will be filled with the list of size 10, filled with numbers from 42 to 53.

Format pattern
^^^^^^^^^^^^^^
One may want to set variables defined anywhere in the configuration file (e.g. a directory path) into a string (e.g. a full file path).
Here is the corresponding syntax:

.. code-block:: yaml

  plots_dir: 'plots'

  ultimate:
    question: f'${plots_dir}/my_plot.png'
    answer: e'"{}/my_plot.png".format(${plots_dir})'

The entry ``(ultimate, question)`` will be filled with the string 'plots/my_plot.png'.
The eval syntax produces the same output in ``(ultimate, answer)`` but is more verbose.


Repeats
^^^^^^^
One can generate on-the-fly configuration with the syntax "$(%)":

.. code-block:: yaml

  main:
    $modules: [model$(1), model$(2)]

  model$(%):
    $modules: [base$(%)]

  base$(%):
    value: e'%$ + 1'

is equivalent to:

.. code-block:: yaml

  main:
    $modules: [model1, model2]

  model1:
    $modules: [base1]

  base1:
    value: 2

  model2:
    $modules: [base2]

  base2:
    value: 3

data_block operations
^^^^^^^^^^^^^^^^^^^^^
We also propose shortcuts for datablock_duplicate, datablock_mapping and datablock_set operations presented above.
These ``data_block`` operations use ``[]`` instead of ``{}`` for ``config_block``.

One can achieve the ``datablock_duplicate`` operation (shallow copy of ``data_block`` entry from ``(section1, name1)`` to ``(section2, name2)``) through the syntax:

.. code-block:: yaml

  ultimate:
    $[section2.name2]: $[section1.name1]

One can achieve the ``datablock_mapping`` operation (``data_block`` entry ``(section2, name2)`` referencing ``(section1, name1)``) through the syntax:

.. code-block:: yaml

  ultimate:
    $[section2.name2]: $&[section1.name1]

Eventually, the ``datablock_set`` operation (locally filling ``data_block`` entry ``(section, name)``) can be achieved with:

.. code-block:: yaml

  answer:
    to: 42

  ultimate:
    $[section.name]: 42

Here 42 can be replaced by any reference to the configuration file (e.g. ``${answer.to}``).
